{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_iYcla4kCX67"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "np.random.seed(1234)\n",
        "tf.random.set_seed(1234)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kgna3kY6CX67",
        "outputId": "0e8fbdbd-0180-4e5a-cd74-8208c4e45ab2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "tf.config.list_physical_devices('GPU')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "JodgHy9nCX68",
        "outputId": "0f6d3ca8-d48a-4bb7-e7ed-32fe2fa0249a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "40960/29515 [=========================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "26435584/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "16384/5148 [===============================================================================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "4431872/4422102 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "(X_train, y_train), (X_test, y_test) = keras.datasets.fashion_mnist.load_data() # Load MNIST or FMNIST\n",
        "assert X_train.shape == (60000, 28, 28)\n",
        "assert X_test.shape == (10000, 28, 28)\n",
        "assert y_train.shape == (60000,)\n",
        "assert y_test.shape == (10000,)\n",
        "\n",
        "\n",
        "# Display randomly selected data\n",
        "indices = list(np.random.randint(X_train.shape[0],size=3))\n",
        "for i in range(3):\n",
        "    plt.subplot(1,3,i+1)\n",
        "    plt.imshow(X_train[indices[i]].reshape(28,28), cmap='gray', interpolation='none')\n",
        "    plt.title(\"Index {} Class {}\".format(indices[i], y_train[indices[i]]))\n",
        "    plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Important\n",
        "* Always have a validation set, the procedure to create validation or dev set is by performing random sample without replacement on train set and then only using that fraction as dev set. \n",
        "* Simple approach is to set some K samples, you can extract them from start, mid or end.\n",
        "* Imagine validation set that partially approximates test set distribution and we assume our model would produce identical results when we test it on test set.\n",
        "* Always optimize your hyperparameters by looking at performance on validation set and not test set.\n",
        "* Do not touch test set, we have this to test how our model would work on unseen data."
      ],
      "metadata": {
        "id": "PowjAHuw-wm7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIRI-uLoCX69",
        "outputId": "dfe688ab-b930-4964-8ede-59327cee8f88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "size of training set is 50000 samples\n",
            "every train example is 28 by 28\n",
            "size of validation set is 10000 samples\n",
            "every validation example is 28 by 28\n",
            "size of training set is 50000 samples\n",
            "every train example has 784 features\n",
            "size of validation set is 10000 samples\n",
            "every validation example has 784 features\n"
          ]
        }
      ],
      "source": [
        "# Split train dataset into train and validation\n",
        "X_val = X_train[50000:60000]\n",
        "X_train = X_train[0:50000]\n",
        "y_val = y_train[50000:60000]\n",
        "y_train = y_train[0:50000]\n",
        "\n",
        "print(\"size of training set is\", str(X_train.shape[0]), \"samples\")\n",
        "print(\"every train example is\", str(X_train.shape[1]), \"by\", str(X_train.shape[2]))\n",
        "\n",
        "print(\"size of validation set is\", str(X_val.shape[0]), \"samples\")\n",
        "print(\"every validation example is\", str(X_val.shape[1]), \"by\", str(X_val.shape[2]))\n",
        "\n",
        "X_train = X_train.reshape(50000, 28*28)\n",
        "X_val = X_val.reshape(10000, 28*28)\n",
        "X_test = X_test.reshape(10000, 28*28)\n",
        "\n",
        "print(\"size of training set is\", str(X_train.shape[0]), \"samples\")\n",
        "print(\"every train example has\", str(X_train.shape[1]), \"features\")\n",
        "\n",
        "print(\"size of validation set is\", str(X_val.shape[0]), \"samples\")\n",
        "print(\"every validation example has\", str(X_val.shape[1]), \"features\")\n",
        "\n",
        "\n",
        "# Split dataset into batches\n",
        "#train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(16)\n",
        "#test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(4)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Points to remember\n",
        "* If using any type of neural network, normalize your input between 0-1.\n",
        "* One can use various procedures to achieve this, divide by largest value (for images we use 255), subtract mean from data and then normalize, one can even augment them and use other steps for normalization.\n",
        "* Normalization is important step, one could observe significant boost in performance just by having better normalization scheme.\n",
        "* For targets we always use one-hot encodings."
      ],
      "metadata": {
        "id": "VdMEIaFKAscU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDyZ8bZjCX69",
        "outputId": "02479511-398e-4662-a2f8-b09b3f3a4b06"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "#Normalize Data\n",
        "\n",
        "X_train = X_train/255\n",
        "X_val = X_val/255\n",
        "X_test = X_test/255\n",
        "# X_train[0]\n",
        "np.max(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lIIy313CX69",
        "outputId": "e2363b97-5123-4fae-b347-c1ec29942c3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([10000    10], shape=(2,), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "size_input = X_train.shape[1]\n",
        "size_hidden1 = 128\n",
        "size_hidden2 = 128\n",
        "size_hidden3 = 128\n",
        "size_output = 10\n",
        "\n",
        "number_of_train_examples = X_train.shape[0]\n",
        "number_of_test_examples = X_test.shape[0]\n",
        "\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes=10) # Other function is tf.one_hot(y_train,depth=10)\n",
        "y_val = tf.keras.utils.to_categorical(y_val, num_classes=10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)\n",
        "print(tf.shape(y_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importance of weight initialization\n",
        "\n",
        "* One reason backprop based models can perform bettter lies with the weight initialization method, one important point one should remember is that, if yur weights are initialized to be too high or low, backprop would struggle.\n",
        "* Hence one should always carefully initialize weights of your model, below i have shown approach with random_normal, one can use random_uniform, truncated version of both, Xavier init and orthogonal. \n",
        "* You will find modern day NNs have achieved stable and better performance by simply switching to better init and majority of cases Xavier or Orthogonal works best.\n",
        "* Always initialize your bias using zero or some small constant (ideally 0.01 or less works better). We use bias to shift the activation and in some cases it can stabalize learning, but having large bias can cause negative results.\n",
        "\n",
        "# Loss function\n",
        "\n",
        "* We will always cross-entropy loss for classification.\n",
        "\n",
        "* tf softmax,\n",
        "loss= tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_pred_tf, labels=y_true_tf)), this function is simply saying that it will calculate softmax for you, simply provide logits to it. \n",
        "\n",
        "* In other output of your forward pass directly goes this function. Now this operator will calculate or apply softmax over prediction or logits and calculate cross-entropy between prediction and target. I am using reduce_mean since we apply this over batches.\n",
        "* Second is using keras\n",
        "Method 1 :- This function requires logits, hence same as above you will pass logits or output variable to this function. Now remember you need from_logits = True, for this to work.\n",
        "cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "loss_x = cce(y_true_tf, y_pred_tf) \n",
        "\n",
        "* Method 2:- In this we will apply softmax to output function and then pass to CCE loss.\n",
        "So the approach is \n",
        "output = tf.nn.softmax(output)\n",
        "cce = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
        "loss_x = cce(y_true_tf, y_pred_tf) "
      ],
      "metadata": {
        "id": "U7KCVarVCVPW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "obN7WPLpCX69"
      },
      "outputs": [],
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3, size_output, batch_norm = \"preact\", alpha_mov = 0.8,device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden1: int, size of the 1st hidden layer\n",
        "    size_hidden2: int, size of the 2nd hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden1, self.size_hidden2, self.size_hidden3, self.size_output, self.batch_norm, self.alpha_mov, self.device =\\\n",
        "    size_input, size_hidden1, size_hidden2, size_hidden3, size_output, batch_norm, alpha_mov, device\n",
        "    \n",
        "    # Initialize weights between input mapping and a layer g(f(x)) = layer\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1],stddev=0.1)) # Xavier(Fan-in fan-out) and Orthogonal\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1])) # 0 or constant(0.01)\n",
        "    self.beta1 = tf.Variable(tf.zeros([1, self.size_hidden1])) #get info about how to initialize\n",
        "    self.gamma1 = tf.Variable(tf.ones([1, self.size_hidden1]))\n",
        "    \n",
        "    # Initialize weights between input layer and 1st hidden layer\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
        "    self.beta2 = tf.Variable(tf.zeros([1, self.size_hidden2])) #get info about how to initialize\n",
        "    self.gamma2 = tf.Variable(tf.ones([1, self.size_hidden2]))\n",
        "    \n",
        "    # Initialize weights between 1st hidden layer and 2nd hidden layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_hidden3],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b3 = tf.Variable(tf.zeros([1, self.size_hidden3]))\n",
        "    self.beta3 = tf.Variable(tf.zeros([1, self.size_hidden3])) #get info about how to initialize\n",
        "    self.gamma3 = tf.Variable(tf.ones([1, self.size_hidden3]))\n",
        "    \n",
        "     # Initialize weights between 2nd hidden layer and output layer\n",
        "    self.W4 = tf.Variable(tf.random.normal([self.size_hidden3, self.size_output],stddev=0.1))\n",
        "    # Initialize biases for output layer\n",
        "    self.b4 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "    \n",
        "    self.epsilon = 1e-7\n",
        "    self.mov_mean = tf.Variable(0.)\n",
        "    self.mov_var = tf.Variable(0.)\n",
        "    self.test_mean = None\n",
        "    self.test_var = None\n",
        "\n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.W4, self.b1, self.b2, self.b3, self.b4, self.beta1, self.beta2, self.beta3, self.gamma1, self.gamma2, self.gamma3]\n",
        "    #self.variables = [self.W1, self.W2, self.W3, self.W4, self.b1, self.b2, self.b3, self.b4, self.beta1, self.gamma1, self.beta2, self.gamma2]\n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        if self.batch_norm == \"preact\":\n",
        "          self.y = self.compute_output_for_preact(X)\n",
        "        else:\n",
        "          self.y = self.compute_output_for_postact(X)\n",
        "    else:\n",
        "        if self.batch_norm == \"preact\":\n",
        "          self.y = self.compute_output_for_preact(X)\n",
        "        else:\n",
        "          self.y = self.compute_output_for_postact(X)\n",
        "      \n",
        "    return self.y\n",
        "\n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    #y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "    loss_x = cce(y_true_tf, y_pred_tf)\n",
        "\n",
        "    #print (f\"Loss : {loss_x}\")\n",
        "    # Use keras or tf_softmax, both should work for any given model\n",
        "    #loss_x = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_pred_tf, labels=y_true_tf))\n",
        "    \n",
        "    return loss_x\n",
        "\n",
        "  def backward(self, X_train, y_train, opti):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = opti\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        \n",
        "      predicted = self.forward(X_train)\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "        \n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "\n",
        "    #print (f\"grads: {grads}\")\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "\n",
        "  def batch_norm_train(self, Z, beta, gamma):\n",
        "    # Z : result of pre or post activation\n",
        "    #Calculating mean and variance\n",
        "    # if test_mean is None then trigger training logic\n",
        "    if self.test_mean is not None:\n",
        "      #print (f\"TEST : Moving avg: {self.mov_mean}, Moving variance: {self.mov_var}\")\n",
        "      mean_mini_batch = self.test_mean\n",
        "      var_mini_batch = self.test_var\n",
        "    else:\n",
        "      mean_mini_batch = tf.reduce_mean(Z)\n",
        "      var_mini_batch = tf.math.reduce_variance(Z)\n",
        "      self.mov_mean.assign(self.alpha_mov * self.mov_mean + (1 - self.alpha_mov) * mean_mini_batch)\n",
        "      self.mov_var.assign(self.alpha_mov * self.mov_var + (1 - self.alpha_mov) * var_mini_batch)\n",
        "    \n",
        "    Z_hat = tf.divide(tf.subtract(Z, mean_mini_batch), tf.sqrt(tf.add(var_mini_batch, self.epsilon)))\n",
        "    batch_norm = tf.add(tf.multiply(gamma, Z_hat), beta)\n",
        "\n",
        "    return batch_norm\n",
        "  \n",
        "  #@tf.function\n",
        "  def compute_output_for_preact(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #X_tf = X\n",
        "    \n",
        "    # Compute values in hidden layers\n",
        "    z1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    z1 = self.batch_norm_train(z1, self.beta1, self.gamma1)\n",
        "    h1 = tf.nn.relu(z1)\n",
        "      \n",
        "    z2 = tf.matmul(h1, self.W2) + self.b2\n",
        "    z2 = self.batch_norm_train(z2, self.beta2, self.gamma2)\n",
        "    h2 = tf.nn.relu(z2)\n",
        "    \n",
        "    z3 = tf.matmul(h2, self.W3) + self.b3\n",
        "    z3 = self.batch_norm_train(z3, self.beta3, self.gamma3)\n",
        "    h3 = tf.nn.relu(z3)\n",
        "\n",
        "    # Compute output\n",
        "    output = tf.matmul(h3, self.W4) + self.b4\n",
        "    \n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this \n",
        "    # Second add tf.Softmax(output) and then return this variable\n",
        "    return (output)\n",
        "  \n",
        "  #@tf.function\n",
        "  def compute_output_for_postact(self, X):\n",
        "      \"\"\"\n",
        "      Custom method to obtain output tensor during forward pass\n",
        "      \"\"\"\n",
        "      # Cast X to float32\n",
        "      X_tf = tf.cast(X, dtype=tf.float32)\n",
        "      #X_tf = X\n",
        "      \n",
        "      # Compute values in hidden layers\n",
        "      z1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "      h1 = tf.nn.relu(z1)\n",
        "      h1 = self.batch_norm_train(h1, self.beta1, self.gamma1)\n",
        "        \n",
        "      z2 = tf.matmul(h1, self.W2) + self.b2\n",
        "      h2 = tf.nn.relu(z2)\n",
        "      h2 = self.batch_norm_train(h2, self.beta2, self.gamma2)\n",
        "      \n",
        "      z3 = tf.matmul(h2, self.W3) + self.b3\n",
        "      h3 = tf.nn.relu(z3)\n",
        "      h3 = self.batch_norm_train(h3, self.beta3, self.gamma3)\n",
        "\n",
        "      # Compute output\n",
        "      output = tf.matmul(h3, self.W4) + self.b4\n",
        "      \n",
        "      #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this \n",
        "      # Second add tf.Softmax(output) and then return this variable\n",
        "      return (output)\n",
        "\n",
        "    \n",
        "\n",
        "#  def stderr(self,y_pred):\n",
        "#     \"\"\"\n",
        "#      Calculate standard error\n",
        "#      \"\"\"\n",
        "#     y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "#     std_dev = np.std(y_pred_tf) #Calculates standard deviation\n",
        "#     std_err = std_dev/sqrt(len(y_pred_tf))\n",
        "#     return std_err \n",
        "\n",
        "\n",
        "  def var(self,y_pred):\n",
        "     \"\"\"\n",
        "      Calculate variance \n",
        "      \"\"\"\n",
        "     y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "     std_dev = np.std(y_pred_tf) #Calculates standard deviation\n",
        "     variance = (std_dev**2) # calculate variance\n",
        "     return variance \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to train a model"
      ],
      "metadata": {
        "id": "MtinH6xf4rnq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(mlp, num_epochs, seed, shuffle_size, batch_size, opti):\n",
        "  time_start = time.time()\n",
        "  train_acc_list = []\n",
        "  val_acc_list = []\n",
        "  train_cce_list = []\n",
        "  for epoch in range(num_epochs):\n",
        "    loss_total = tf.zeros([1,1], dtype=tf.float32)    \n",
        "    train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(shuffle_size, seed=epoch*(seed)).batch(batch_size)\n",
        "    kz = 0\n",
        "    accuracy_z = 0.0\n",
        "    cur_train_acc = 0.0\n",
        "    for inputs, outputs in train_ds:\n",
        "      qw, tr = tf.shape(inputs)\n",
        "      kz = kz + 1\n",
        "      preds = mlp.forward(inputs) \n",
        "      loss_total = loss_total + mlp.loss(preds, outputs)\n",
        "      mlp.backward(inputs, outputs, opti)\n",
        "\n",
        "    preds = mlp.forward(X_train)\n",
        "    # Get probs, remember we only have logits from our forward function, we need to apply softmax on top of it to get probs\n",
        "    preds = tf.nn.softmax(preds)\n",
        "    correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y_train, 1))\n",
        "    accuracy_z = accuracy_z + tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "    cur_train_acc += accuracy_z.numpy()\n",
        "    ds = cur_train_acc\n",
        "    train_acc_list.append(ds)\n",
        "    print('\\nTrain Accuracy: {:.4f}'.format(ds))\n",
        "\n",
        "    avg_cce = np.sum(loss_total) / X_train.shape[0]\n",
        "    train_cce_list.append(avg_cce)\n",
        "    print('Number of Epoch = {} - Average Cross Entropy:= {} '.format(epoch + 1, avg_cce))\n",
        "\n",
        "    # Setting mean and variance for validation set accuracy check\n",
        "    mlp.test_mean = mlp.mov_mean\n",
        "    mlp.test_var = mlp.mov_var\n",
        "    preds_val = mlp.forward(X_val)\n",
        "    preds_val = tf.nn.softmax(preds_val)\n",
        "    correct_prediction = tf.equal(tf.argmax(preds_val, 1), tf.argmax(y_val, 1))\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "    cur_val_acc = accuracy.numpy()\n",
        "    val_acc_list.append(cur_val_acc)\n",
        "    print('\\nValidation Accuracy: {:.4f}'.format(cur_val_acc))\n",
        "\n",
        "    # Setting mean and variance to None for training Batch Norm logic to trigger\n",
        "    mlp.test_mean = None\n",
        "    mlp.test_var = None\n",
        "    #plt.plot(epoch + 1, np.sum(loss_total) / X_train.shape[0], 'go')\n",
        "        \n",
        "  time_taken = time.time() - time_start\n",
        "      \n",
        "  # Validate model\n",
        "  print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "  return time_taken, train_acc_list, val_acc_list, train_cce_list\n",
        "  #For per epoch_time = Total_Time / Number_of_epochs"
      ],
      "metadata": {
        "id": "eUB7-cVR83Zk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function to test a model"
      ],
      "metadata": {
        "id": "0DQpTC7z9w2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model):\n",
        "# Initialize\n",
        "  test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "  correct_prediction = tf.Variable(0, dtype=tf.float32)\n",
        "  #test_loss_total = 0.0\n",
        "  for inputs, outputs in test_ds:\n",
        "    preds = model.forward(inputs)\n",
        "    test_loss_total = test_loss_total + model.loss(preds, outputs)\n",
        "  \n",
        "  avg_test_loss = np.sum(test_loss_total.numpy()) / X_test.shape[0]\n",
        "  print('Test loss: {:.4f}'.format(avg_test_loss))\n",
        "\n",
        "  # Test model\n",
        "  preds_test = model.forward(X_test)\n",
        "  preds_test = tf.nn.softmax(preds_test)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds_test, 1), tf.argmax(y_test, 1))\n",
        "\n",
        "  # Calculate accuracy\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_test_acc = accuracy.numpy()\n",
        "  print('\\nTest Accuracy: {:.2f}'.format(cur_test_acc))\n",
        "  return avg_test_loss, cur_test_acc"
      ],
      "metadata": {
        "id": "nnAvgZIV9zaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_EPOCHS = 10\n",
        "seed = 5308\n",
        "shuffle_size = 25\n",
        "batch_size=128\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)"
      ],
      "metadata": {
        "id": "uMzQBugp9ohM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training - SGD preact"
      ],
      "metadata": {
        "id": "dabnnE2E-Pj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mlp_preact = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, batch_norm=\"preact\", device='gpu')\n",
        "opti = tf.keras.optimizers.SGD(learning_rate = 0.1, momentum=0.9)\n",
        "time_to_train, train_acclist, val_acclist, train_ccelist  = train_model(mlp_preact, NUM_EPOCHS, seed = seed, shuffle_size=shuffle_size, batch_size=batch_size, opti=opti)"
      ],
      "metadata": {
        "id": "04EX5s5s6_PF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02c3e920-7786-4934-9d66-8f163a1af194"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Accuracy: 0.8435\n",
            "Number of Epoch = 1 - Average Cross Entropy:= 0.00437664794921875 \n",
            "\n",
            "Validation Accuracy: 0.8307\n",
            "\n",
            "Train Accuracy: 0.8690\n",
            "Number of Epoch = 2 - Average Cross Entropy:= 0.003115779113769531 \n",
            "\n",
            "Validation Accuracy: 0.8571\n",
            "\n",
            "Train Accuracy: 0.8787\n",
            "Number of Epoch = 3 - Average Cross Entropy:= 0.0027889276123046875 \n",
            "\n",
            "Validation Accuracy: 0.8606\n",
            "\n",
            "Train Accuracy: 0.8884\n",
            "Number of Epoch = 4 - Average Cross Entropy:= 0.00261331298828125 \n",
            "\n",
            "Validation Accuracy: 0.8700\n",
            "\n",
            "Train Accuracy: 0.8922\n",
            "Number of Epoch = 5 - Average Cross Entropy:= 0.002459342041015625 \n",
            "\n",
            "Validation Accuracy: 0.8721\n",
            "\n",
            "Train Accuracy: 0.8986\n",
            "Number of Epoch = 6 - Average Cross Entropy:= 0.002328775634765625 \n",
            "\n",
            "Validation Accuracy: 0.8790\n",
            "\n",
            "Train Accuracy: 0.8935\n",
            "Number of Epoch = 7 - Average Cross Entropy:= 0.002243583221435547 \n",
            "\n",
            "Validation Accuracy: 0.8737\n",
            "\n",
            "Train Accuracy: 0.8961\n",
            "Number of Epoch = 8 - Average Cross Entropy:= 0.002145598297119141 \n",
            "\n",
            "Validation Accuracy: 0.8765\n",
            "\n",
            "Train Accuracy: 0.8942\n",
            "Number of Epoch = 9 - Average Cross Entropy:= 0.0021234954833984376 \n",
            "\n",
            "Validation Accuracy: 0.8765\n",
            "\n",
            "Train Accuracy: 0.8999\n",
            "Number of Epoch = 10 - Average Cross Entropy:= 0.002034118499755859 \n",
            "\n",
            "Validation Accuracy: 0.8781\n",
            "\n",
            "Total time taken (in seconds): 116.94\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check inference - preact"
      ],
      "metadata": {
        "id": "6pIRJbB3A7fR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking preact\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(1000)\n",
        "mlp_preact.test_mean = mlp_preact.mov_mean\n",
        "mlp_preact.test_var = mlp_preact.mov_var\n",
        "test_model(mlp_preact)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8XtVkr0A9YO",
        "outputId": "4ecb79c8-ef09-46c3-ed60-04b3995cfdc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.0004\n",
            "\n",
            "Test Accuracy: 0.87\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0003645068407058716, 0.871)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run training - SGD postact"
      ],
      "metadata": {
        "id": "my-JVu8cBky_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mlp_postact = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, batch_norm=\"postact\", device='gpu')\n",
        "opti = tf.keras.optimizers.SGD(learning_rate = 0.1, momentum=0.9)\n",
        "time_to_train, train_acclist, val_acclist, train_ccelist  = train_model(mlp_postact, NUM_EPOCHS, seed = seed, shuffle_size=shuffle_size, batch_size=batch_size, opti=opti)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhKtR1DaBoPO",
        "outputId": "579c4a45-2773-4243-c5d2-774cbd3130ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Accuracy: 0.8508\n",
            "Number of Epoch = 1 - Average Cross Entropy:= 0.0042972244262695315 \n",
            "\n",
            "Validation Accuracy: 0.7970\n",
            "\n",
            "Train Accuracy: 0.8735\n",
            "Number of Epoch = 2 - Average Cross Entropy:= 0.0030968853759765626 \n",
            "\n",
            "Validation Accuracy: 0.8499\n",
            "\n",
            "Train Accuracy: 0.8851\n",
            "Number of Epoch = 3 - Average Cross Entropy:= 0.002770556640625 \n",
            "\n",
            "Validation Accuracy: 0.8577\n",
            "\n",
            "Train Accuracy: 0.8869\n",
            "Number of Epoch = 4 - Average Cross Entropy:= 0.0025629177856445313 \n",
            "\n",
            "Validation Accuracy: 0.8602\n",
            "\n",
            "Train Accuracy: 0.8957\n",
            "Number of Epoch = 5 - Average Cross Entropy:= 0.0024008526611328123 \n",
            "\n",
            "Validation Accuracy: 0.8629\n",
            "\n",
            "Train Accuracy: 0.9030\n",
            "Number of Epoch = 6 - Average Cross Entropy:= 0.0022762193298339842 \n",
            "\n",
            "Validation Accuracy: 0.8739\n",
            "\n",
            "Train Accuracy: 0.9038\n",
            "Number of Epoch = 7 - Average Cross Entropy:= 0.0021766017150878906 \n",
            "\n",
            "Validation Accuracy: 0.8662\n",
            "\n",
            "Train Accuracy: 0.9037\n",
            "Number of Epoch = 8 - Average Cross Entropy:= 0.0020636444091796874 \n",
            "\n",
            "Validation Accuracy: 0.8666\n",
            "\n",
            "Train Accuracy: 0.8993\n",
            "Number of Epoch = 9 - Average Cross Entropy:= 0.0019940647888183594 \n",
            "\n",
            "Validation Accuracy: 0.8593\n",
            "\n",
            "Train Accuracy: 0.9031\n",
            "Number of Epoch = 10 - Average Cross Entropy:= 0.0019300379943847656 \n",
            "\n",
            "Validation Accuracy: 0.8677\n",
            "\n",
            "Total time taken (in seconds): 114.26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference - Post activation model"
      ],
      "metadata": {
        "id": "bKZ8iiHcCoPp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking postact inference\n",
        "mlp_postact.test_mean = mlp_postact.mov_mean\n",
        "mlp_postact.test_var = mlp_postact.mov_var\n",
        "test_model(mlp_postact)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8CHjwLQCnyt",
        "outputId": "5b2efb1e-82dc-478a-fa08-64558c02facc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.0004\n",
            "\n",
            "Test Accuracy: 0.86\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0003953143119812012, 0.8645)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving your model weights or parameters\n",
        "* It is always advisable to save your model checkpoints every k epochs. Look at Saver object in tensorflow/keras.\n",
        "* Visualize your model performance using tensorboard.\n",
        "\n",
        "# Steps to save model weights using pickle\n",
        "* Save your model(trainable variables), in our case self.variables into a pickle file.\n",
        "* Load saved file\n",
        "* Redefine model\n",
        "* Load weights\n",
        "* Re-train or test your model\n"
      ],
      "metadata": {
        "id": "2O5EAiXOHQir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "import pickle\n",
        "variables_old = mlp_preact.variables\n",
        "with open('weights_preact.pickle', 'wb') as handle:\n",
        "    pickle.dump(variables_old, handle)"
      ],
      "metadata": {
        "id": "-gtwqTXnjW30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('weights.pickle', 'rb') as handle:\n",
        "    b = pickle.load(handle)"
      ],
      "metadata": {
        "id": "l9Jc34APkghg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reusing inference code form previous assignment\n"
      ],
      "metadata": {
        "id": "76wd2DpFRuBm"
      }
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "5DRomYsUCX6_"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtaining inferences for all three models - 10 iteration per model\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "############## Setting values as per dataset ##############\n",
        "size_input = 784\n",
        "size_hidden_1 = 128 \n",
        "size_hidden_2 = 128\n",
        "side_hidden_3 = 128\n",
        "size_output = 10 \n",
        "number_of_train_examples = X_train.shape[0]\n",
        "number_of_test_examples = X_test.shape[0]\n",
        "print (f\"{number_of_test_examples},{number_of_train_examples}\")\n",
        "\n",
        "########################################################\n",
        "'''\n",
        "Additional steps done above\n",
        "1. Reshape to flatten\n",
        "2. Normalization\n",
        "3. One hot encoding\n",
        "'''\n",
        "\n",
        "#######################################################\n",
        "#reg_type_list = [\"default\", \"L2\", \"dropout\"]\n",
        "reg_type_list = [\"preact\", \"postact\"]\n",
        "\n",
        "inference_stats_total_dict = {\n",
        "    #\"custom\":{},\n",
        "    #\"Adam\":{},\n",
        "    #\"RMSprop\":{},\n",
        "    \"SGD\":{}\n",
        "}\n",
        "\n",
        "\n",
        "NUM_EPOCHS = 10 # reduced to 10 to save overall time\n",
        "#NUM_EPOCHS = 2\n",
        "random_seeds = list(np.random.randint(low=1000, high=100000, size=3))\n",
        "#random_seeds = list(np.random.randint(low=2000, high=4000, size=2))\n",
        "print (f\"Random seeds used in the test : {random_seeds}\")\n",
        "inf_iters = len(random_seeds)\n",
        "device_type = 'gpu'\n",
        "shuffle_size = 25\n",
        "batch_size = 128\n",
        "#batch_size = 512\n",
        "learning_rates = {\n",
        "    \"SGD\": 0.1,\n",
        "    #\"Adam\": 0.001,\n",
        "    #\"RMSprop\": 0.001,\n",
        "    #\"custom\": 0.001\n",
        "}\n",
        "\n",
        "\n",
        "optimizers = learning_rates.keys()\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(256)\n",
        "opti = tf.keras.optimizers.SGD(learning_rate = 0.1, momentum=0.9)\n",
        "####################################################\n",
        "\n",
        "for optimizer in optimizers:\n",
        "  inference_stats_dict = {\n",
        "    \"preact\":{}, \n",
        "    \"postact\": {}, \n",
        "  }\n",
        "  print (f\"#####Running trials for optimizer: {optimizer.upper()}#####\")\n",
        "  for reg_type in reg_type_list:\n",
        "    print (f\"## START: Current Model: {reg_type} ##\")\n",
        "    test_cce_list = []\n",
        "    test_acc_list = []\n",
        "    cpu_time_list = []\n",
        "    train_acc_list_dict = {}\n",
        "    val_acc_list_dict = {}\n",
        "    train_cce_list_dict = {}\n",
        "\n",
        "    for iter in tqdm(range(inf_iters)):\n",
        "      print(f\"\\n** Inference Iteration: {iter} **\")\n",
        "      cur_seed = random_seeds[iter]\n",
        "      print (f\"\\n#Training {reg_type} model, optimizer {optimizer} with seed {cur_seed}#\")\n",
        "      np.random.seed(cur_seed)\n",
        "      tf.random.set_seed(cur_seed)\n",
        "\n",
        "\n",
        "      # Model initialization as per regularization type - regularization code triggers only if knob is on\n",
        "      mlp = MLP(size_input, size_hidden_1, size_hidden_2, side_hidden_3, size_output, batch_norm = reg_type, device=device_type)\n",
        "      mlp.test_mean = None\n",
        "      mlp.test_var = None\n",
        "      cputime, train_acc_list, val_acc_list, train_cce_list = \\\n",
        "                train_model(mlp, NUM_EPOCHS, seed=cur_seed, shuffle_size=shuffle_size, batch_size=batch_size, opti=opti)\n",
        "\n",
        "      train_acc_list_dict[cur_seed] = train_acc_list\n",
        "      val_acc_list_dict[cur_seed] = val_acc_list\n",
        "      train_cce_list_dict[cur_seed] = train_cce_list\n",
        "      cpu_time_list.append(cputime)\n",
        "\n",
        "      # Testing the  - Setting test mean and variance to running stats obtained from testing\n",
        "      mlp.test_mean = mlp.mov_mean\n",
        "      mlp.test_var = mlp.mov_var\n",
        "      (cce_test, acc_test) = test_model(mlp)\n",
        "      print(f\"seed: {cur_seed}, Test Cross Entropy loss: {cce_test}, Accuracy: {acc_test}\")\n",
        "\n",
        "      \n",
        "      test_cce_list.append(cce_test)\n",
        "      test_acc_list.append(acc_test)\n",
        "\n",
        "    inference_stats_dict[reg_type][\"test_cce_list\"] = test_cce_list\n",
        "    inference_stats_dict[reg_type][\"test_acc_list\"] = test_acc_list\n",
        "    inference_stats_dict[reg_type][\"cputime_list\"] = cpu_time_list\n",
        "    inference_stats_dict[reg_type][\"train_acc_list_dict\"] = train_acc_list_dict\n",
        "    inference_stats_dict[reg_type][\"val_acc_list_dict\"] = val_acc_list_dict\n",
        "    inference_stats_dict[reg_type][\"train_cce_list_dict\"] = train_cce_list_dict\n",
        "\n",
        "    print (f\"## END: Current Model: {reg_type}##\")\n",
        "    print (f\"Current inference results: \\n{inference_stats_dict}\")\n",
        "  print (f\"Inference stat dict for optimizer : {optimizer}: \\n{inference_stats_dict}\")\n",
        "  inference_stats_total_dict[optimizer] = inference_stats_dict\n",
        "  print (f\"Current status of dictionary: {inference_stats_total_dict}\")\n",
        "  with open('fmnist_stats_bnorm.json', 'w') as convert_file:\n",
        "     convert_file.write(json.dumps(str(inference_stats_total_dict)))\n",
        "  ################################   "
      ],
      "metadata": {
        "id": "BVrziEW8_3RC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34b3f9bd-770f-4146-80e2-4691df3dc347"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000,50000\n",
            "Random seeds used in the test : [77001, 52755, 81065]\n",
            "#####Running trials for optimizer: SGD#####\n",
            "## START: Current Model: preact ##\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/3 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "** Inference Iteration: 0 **\n",
            "\n",
            "#Training preact model, optimizer SGD with seed 77001#\n",
            "\n",
            "Train Accuracy: 0.8468\n",
            "Number of Epoch = 1 - Average Cross Entropy:= 0.004317352294921875 \n",
            "\n",
            "Validation Accuracy: 0.8427\n",
            "\n",
            "Train Accuracy: 0.8715\n",
            "Number of Epoch = 2 - Average Cross Entropy:= 0.0031137924194335935 \n",
            "\n",
            "Validation Accuracy: 0.8579\n",
            "\n",
            "Train Accuracy: 0.8831\n",
            "Number of Epoch = 3 - Average Cross Entropy:= 0.0027720809936523437 \n",
            "\n",
            "Validation Accuracy: 0.8663\n",
            "\n",
            "Train Accuracy: 0.8888\n",
            "Number of Epoch = 4 - Average Cross Entropy:= 0.0025820327758789064 \n",
            "\n",
            "Validation Accuracy: 0.8721\n",
            "\n",
            "Train Accuracy: 0.8924\n",
            "Number of Epoch = 5 - Average Cross Entropy:= 0.0024218844604492187 \n",
            "\n",
            "Validation Accuracy: 0.8787\n",
            "\n",
            "Train Accuracy: 0.8922\n",
            "Number of Epoch = 6 - Average Cross Entropy:= 0.0023418994140625 \n",
            "\n",
            "Validation Accuracy: 0.8719\n",
            "\n",
            "Train Accuracy: 0.9010\n",
            "Number of Epoch = 7 - Average Cross Entropy:= 0.0022235017395019533 \n",
            "\n",
            "Validation Accuracy: 0.8807\n",
            "\n",
            "Train Accuracy: 0.8993\n",
            "Number of Epoch = 8 - Average Cross Entropy:= 0.0021457798767089845 \n",
            "\n",
            "Validation Accuracy: 0.8793\n",
            "\n",
            "Train Accuracy: 0.9000\n",
            "Number of Epoch = 9 - Average Cross Entropy:= 0.0020938906860351564 \n",
            "\n",
            "Validation Accuracy: 0.8832\n",
            "\n",
            "Train Accuracy: 0.9093\n",
            "Number of Epoch = 10 - Average Cross Entropy:= 0.002014486389160156 \n",
            "\n",
            "Validation Accuracy: 0.8840\n",
            "\n",
            "Total time taken (in seconds): 113.80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 33%|      | 1/3 [01:54<03:48, 114.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.0014\n",
            "\n",
            "Test Accuracy: 0.87\n",
            "seed: 77001, Test Cross Entropy loss: 0.0013803061485290528, Accuracy: 0.8744000196456909\n",
            "\n",
            "** Inference Iteration: 1 **\n",
            "\n",
            "#Training preact model, optimizer SGD with seed 52755#\n",
            "\n",
            "Train Accuracy: 0.8339\n",
            "Number of Epoch = 1 - Average Cross Entropy:= 0.004350304870605469 \n",
            "\n",
            "Validation Accuracy: 0.8304\n",
            "\n",
            "Train Accuracy: 0.8626\n",
            "Number of Epoch = 2 - Average Cross Entropy:= 0.0031148712158203125 \n",
            "\n",
            "Validation Accuracy: 0.8507\n",
            "\n",
            "Train Accuracy: 0.8762\n",
            "Number of Epoch = 3 - Average Cross Entropy:= 0.00279951416015625 \n",
            "\n",
            "Validation Accuracy: 0.8594\n",
            "\n",
            "Train Accuracy: 0.8880\n",
            "Number of Epoch = 4 - Average Cross Entropy:= 0.0025889529418945314 \n",
            "\n",
            "Validation Accuracy: 0.8697\n",
            "\n",
            "Train Accuracy: 0.8952\n",
            "Number of Epoch = 5 - Average Cross Entropy:= 0.0024665768432617185 \n",
            "\n",
            "Validation Accuracy: 0.8765\n",
            "\n",
            "Train Accuracy: 0.8980\n",
            "Number of Epoch = 6 - Average Cross Entropy:= 0.002337578582763672 \n",
            "\n",
            "Validation Accuracy: 0.8786\n",
            "\n",
            "Train Accuracy: 0.8969\n",
            "Number of Epoch = 7 - Average Cross Entropy:= 0.002248544769287109 \n",
            "\n",
            "Validation Accuracy: 0.8798\n",
            "\n",
            "Train Accuracy: 0.8977\n",
            "Number of Epoch = 8 - Average Cross Entropy:= 0.0021396717834472655 \n",
            "\n",
            "Validation Accuracy: 0.8813\n",
            "\n",
            "Train Accuracy: 0.8932\n",
            "Number of Epoch = 9 - Average Cross Entropy:= 0.002056458435058594 \n",
            "\n",
            "Validation Accuracy: 0.8742\n",
            "\n",
            "Train Accuracy: 0.9023\n",
            "Number of Epoch = 10 - Average Cross Entropy:= 0.0019872297668457033 \n",
            "\n",
            "Validation Accuracy: 0.8811\n",
            "\n",
            "Total time taken (in seconds): 113.62\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 67%|   | 2/3 [03:47<01:53, 113.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.0014\n",
            "\n",
            "Test Accuracy: 0.87\n",
            "seed: 52755, Test Cross Entropy loss: 0.0014349761009216308, Accuracy: 0.8730999827384949\n",
            "\n",
            "** Inference Iteration: 2 **\n",
            "\n",
            "#Training preact model, optimizer SGD with seed 81065#\n",
            "\n",
            "Train Accuracy: 0.8457\n",
            "Number of Epoch = 1 - Average Cross Entropy:= 0.004414635620117188 \n",
            "\n",
            "Validation Accuracy: 0.8416\n",
            "\n",
            "Train Accuracy: 0.8732\n",
            "Number of Epoch = 2 - Average Cross Entropy:= 0.003104010009765625 \n",
            "\n",
            "Validation Accuracy: 0.8597\n",
            "\n",
            "Train Accuracy: 0.8793\n",
            "Number of Epoch = 3 - Average Cross Entropy:= 0.0028212384033203127 \n",
            "\n",
            "Validation Accuracy: 0.8665\n",
            "\n",
            "Train Accuracy: 0.8848\n",
            "Number of Epoch = 4 - Average Cross Entropy:= 0.0026139642333984376 \n",
            "\n",
            "Validation Accuracy: 0.8681\n",
            "\n",
            "Train Accuracy: 0.8921\n",
            "Number of Epoch = 5 - Average Cross Entropy:= 0.0024749552917480468 \n",
            "\n",
            "Validation Accuracy: 0.8747\n",
            "\n",
            "Train Accuracy: 0.8987\n",
            "Number of Epoch = 6 - Average Cross Entropy:= 0.0023675563049316405 \n",
            "\n",
            "Validation Accuracy: 0.8763\n",
            "\n",
            "Train Accuracy: 0.9009\n",
            "Number of Epoch = 7 - Average Cross Entropy:= 0.002251977996826172 \n",
            "\n",
            "Validation Accuracy: 0.8782\n",
            "\n",
            "Train Accuracy: 0.8927\n",
            "Number of Epoch = 8 - Average Cross Entropy:= 0.0021700347900390623 \n",
            "\n",
            "Validation Accuracy: 0.8756\n",
            "\n",
            "Train Accuracy: 0.9034\n",
            "Number of Epoch = 9 - Average Cross Entropy:= 0.002065221862792969 \n",
            "\n",
            "Validation Accuracy: 0.8786\n",
            "\n",
            "Train Accuracy: 0.8995\n",
            "Number of Epoch = 10 - Average Cross Entropy:= 0.0019999238586425783 \n",
            "\n",
            "Validation Accuracy: 0.8779\n",
            "\n",
            "Total time taken (in seconds): 113.44\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 3/3 [05:41<00:00, 113.91s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.0014\n",
            "\n",
            "Test Accuracy: 0.87\n",
            "seed: 81065, Test Cross Entropy loss: 0.0014285715103149414, Accuracy: 0.8680999875068665\n",
            "## END: Current Model: preact##\n",
            "Current inference results: \n",
            "{'preact': {'test_cce_list': [0.0013803061485290528, 0.0014349761009216308, 0.0014285715103149414], 'test_acc_list': [0.8744, 0.8731, 0.8681], 'cputime_list': [113.8013436794281, 113.61838626861572, 113.44157814979553], 'train_acc_list_dict': {77001: [0.8468199968338013, 0.8715000152587891, 0.8831400275230408, 0.8888400197029114, 0.8923799991607666, 0.8921599984169006, 0.9009600281715393, 0.8993200063705444, 0.8999800086021423, 0.909280002117157], 52755: [0.833899974822998, 0.8626199960708618, 0.8762000203132629, 0.8879799842834473, 0.8952000141143799, 0.8979799747467041, 0.8968799710273743, 0.8976799845695496, 0.8931599855422974, 0.9022799730300903], 81065: [0.8457199931144714, 0.8731600046157837, 0.8792999982833862, 0.8847799897193909, 0.8920999765396118, 0.8986999988555908, 0.9008600115776062, 0.89274001121521, 0.9033600091934204, 0.8994600176811218]}, 'val_acc_list_dict': {77001: [0.8427, 0.8579, 0.8663, 0.8721, 0.8787, 0.8719, 0.8807, 0.8793, 0.8832, 0.884], 52755: [0.8304, 0.8507, 0.8594, 0.8697, 0.8765, 0.8786, 0.8798, 0.8813, 0.8742, 0.8811], 81065: [0.8416, 0.8597, 0.8665, 0.8681, 0.8747, 0.8763, 0.8782, 0.8756, 0.8786, 0.8779]}, 'train_cce_list_dict': {77001: [0.004317352294921875, 0.0031137924194335935, 0.0027720809936523437, 0.0025820327758789064, 0.0024218844604492187, 0.0023418994140625, 0.0022235017395019533, 0.0021457798767089845, 0.0020938906860351564, 0.002014486389160156], 52755: [0.004350304870605469, 0.0031148712158203125, 0.00279951416015625, 0.0025889529418945314, 0.0024665768432617185, 0.002337578582763672, 0.002248544769287109, 0.0021396717834472655, 0.002056458435058594, 0.0019872297668457033], 81065: [0.004414635620117188, 0.003104010009765625, 0.0028212384033203127, 0.0026139642333984376, 0.0024749552917480468, 0.0023675563049316405, 0.002251977996826172, 0.0021700347900390623, 0.002065221862792969, 0.0019999238586425783]}}, 'postact': {}}\n",
            "## START: Current Model: postact ##\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/3 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "** Inference Iteration: 0 **\n",
            "\n",
            "#Training postact model, optimizer SGD with seed 77001#\n",
            "\n",
            "Train Accuracy: 0.8405\n",
            "Number of Epoch = 1 - Average Cross Entropy:= 0.00470559814453125 \n",
            "\n",
            "Validation Accuracy: 0.7907\n",
            "\n",
            "Train Accuracy: 0.8661\n",
            "Number of Epoch = 2 - Average Cross Entropy:= 0.0031426089477539065 \n",
            "\n",
            "Validation Accuracy: 0.8252\n",
            "\n",
            "Train Accuracy: 0.8764\n",
            "Number of Epoch = 3 - Average Cross Entropy:= 0.0028356143188476564 \n",
            "\n",
            "Validation Accuracy: 0.8440\n",
            "\n",
            "Train Accuracy: 0.8823\n",
            "Number of Epoch = 4 - Average Cross Entropy:= 0.002629926452636719 \n",
            "\n",
            "Validation Accuracy: 0.8547\n",
            "\n",
            "Train Accuracy: 0.8925\n",
            "Number of Epoch = 5 - Average Cross Entropy:= 0.002500733184814453 \n",
            "\n",
            "Validation Accuracy: 0.8673\n",
            "\n",
            "Train Accuracy: 0.8939\n",
            "Number of Epoch = 6 - Average Cross Entropy:= 0.0023918373107910157 \n",
            "\n",
            "Validation Accuracy: 0.8680\n",
            "\n",
            "Train Accuracy: 0.8962\n",
            "Number of Epoch = 7 - Average Cross Entropy:= 0.002278388366699219 \n",
            "\n",
            "Validation Accuracy: 0.8665\n",
            "\n",
            "Train Accuracy: 0.9014\n",
            "Number of Epoch = 8 - Average Cross Entropy:= 0.0021790437316894532 \n",
            "\n",
            "Validation Accuracy: 0.8670\n",
            "\n",
            "Train Accuracy: 0.9012\n",
            "Number of Epoch = 9 - Average Cross Entropy:= 0.0021119049072265625 \n",
            "\n",
            "Validation Accuracy: 0.8743\n",
            "\n",
            "Train Accuracy: 0.9007\n",
            "Number of Epoch = 10 - Average Cross Entropy:= 0.0020265940856933596 \n",
            "\n",
            "Validation Accuracy: 0.8765\n",
            "\n",
            "Total time taken (in seconds): 113.14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 33%|      | 1/3 [01:53<03:46, 113.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.0015\n",
            "\n",
            "Test Accuracy: 0.87\n",
            "seed: 77001, Test Cross Entropy loss: 0.0014517539978027344, Accuracy: 0.8695999979972839\n",
            "\n",
            "** Inference Iteration: 1 **\n",
            "\n",
            "#Training postact model, optimizer SGD with seed 52755#\n",
            "\n",
            "Train Accuracy: 0.8472\n",
            "Number of Epoch = 1 - Average Cross Entropy:= 0.00434407958984375 \n",
            "\n",
            "Validation Accuracy: 0.8137\n",
            "\n",
            "Train Accuracy: 0.8681\n",
            "Number of Epoch = 2 - Average Cross Entropy:= 0.00311839111328125 \n",
            "\n",
            "Validation Accuracy: 0.8410\n",
            "\n",
            "Train Accuracy: 0.8826\n",
            "Number of Epoch = 3 - Average Cross Entropy:= 0.0028050027465820315 \n",
            "\n",
            "Validation Accuracy: 0.8547\n",
            "\n",
            "Train Accuracy: 0.8898\n",
            "Number of Epoch = 4 - Average Cross Entropy:= 0.002594745178222656 \n",
            "\n",
            "Validation Accuracy: 0.8622\n",
            "\n",
            "Train Accuracy: 0.8894\n",
            "Number of Epoch = 5 - Average Cross Entropy:= 0.0024711099243164064 \n",
            "\n",
            "Validation Accuracy: 0.8578\n",
            "\n",
            "Train Accuracy: 0.8967\n",
            "Number of Epoch = 6 - Average Cross Entropy:= 0.0023571014404296874 \n",
            "\n",
            "Validation Accuracy: 0.8675\n",
            "\n",
            "Train Accuracy: 0.8994\n",
            "Number of Epoch = 7 - Average Cross Entropy:= 0.002234974365234375 \n",
            "\n",
            "Validation Accuracy: 0.8676\n",
            "\n",
            "Train Accuracy: 0.9024\n",
            "Number of Epoch = 8 - Average Cross Entropy:= 0.002129510955810547 \n",
            "\n",
            "Validation Accuracy: 0.8741\n",
            "\n",
            "Train Accuracy: 0.9038\n",
            "Number of Epoch = 9 - Average Cross Entropy:= 0.002051008453369141 \n",
            "\n",
            "Validation Accuracy: 0.8767\n",
            "\n",
            "Train Accuracy: 0.9016\n",
            "Number of Epoch = 10 - Average Cross Entropy:= 0.00197765869140625 \n",
            "\n",
            "Validation Accuracy: 0.8736\n",
            "\n",
            "Total time taken (in seconds): 112.92\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 67%|   | 2/3 [03:46<01:53, 113.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.0015\n",
            "\n",
            "Test Accuracy: 0.86\n",
            "seed: 52755, Test Cross Entropy loss: 0.0015488698959350586, Accuracy: 0.8641999959945679\n",
            "\n",
            "** Inference Iteration: 2 **\n",
            "\n",
            "#Training postact model, optimizer SGD with seed 81065#\n",
            "\n",
            "Train Accuracy: 0.8521\n",
            "Number of Epoch = 1 - Average Cross Entropy:= 0.004507053527832031 \n",
            "\n",
            "Validation Accuracy: 0.7941\n",
            "\n",
            "Train Accuracy: 0.8741\n",
            "Number of Epoch = 2 - Average Cross Entropy:= 0.0031397256469726563 \n",
            "\n",
            "Validation Accuracy: 0.8209\n",
            "\n",
            "Train Accuracy: 0.8789\n",
            "Number of Epoch = 3 - Average Cross Entropy:= 0.0028085311889648437 \n",
            "\n",
            "Validation Accuracy: 0.8268\n",
            "\n",
            "Train Accuracy: 0.8889\n",
            "Number of Epoch = 4 - Average Cross Entropy:= 0.0026214443969726563 \n",
            "\n",
            "Validation Accuracy: 0.8479\n",
            "\n",
            "Train Accuracy: 0.8895\n",
            "Number of Epoch = 5 - Average Cross Entropy:= 0.002468592529296875 \n",
            "\n",
            "Validation Accuracy: 0.8454\n",
            "\n",
            "Train Accuracy: 0.8960\n",
            "Number of Epoch = 6 - Average Cross Entropy:= 0.002346569061279297 \n",
            "\n",
            "Validation Accuracy: 0.8562\n",
            "\n",
            "Train Accuracy: 0.8973\n",
            "Number of Epoch = 7 - Average Cross Entropy:= 0.0022278785705566405 \n",
            "\n",
            "Validation Accuracy: 0.8629\n",
            "\n",
            "Train Accuracy: 0.8958\n",
            "Number of Epoch = 8 - Average Cross Entropy:= 0.0021207963562011717 \n",
            "\n",
            "Validation Accuracy: 0.8622\n",
            "\n",
            "Train Accuracy: 0.9023\n",
            "Number of Epoch = 9 - Average Cross Entropy:= 0.0020305458068847655 \n",
            "\n",
            "Validation Accuracy: 0.8719\n",
            "\n",
            "Train Accuracy: 0.8995\n",
            "Number of Epoch = 10 - Average Cross Entropy:= 0.001962476501464844 \n",
            "\n",
            "Validation Accuracy: 0.8666\n",
            "\n",
            "Total time taken (in seconds): 113.21\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 3/3 [05:40<00:00, 113.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.0016\n",
            "\n",
            "Test Accuracy: 0.86\n",
            "seed: 81065, Test Cross Entropy loss: 0.0015667022705078124, Accuracy: 0.8622000217437744\n",
            "## END: Current Model: postact##\n",
            "Current inference results: \n",
            "{'preact': {'test_cce_list': [0.0013803061485290528, 0.0014349761009216308, 0.0014285715103149414], 'test_acc_list': [0.8744, 0.8731, 0.8681], 'cputime_list': [113.8013436794281, 113.61838626861572, 113.44157814979553], 'train_acc_list_dict': {77001: [0.8468199968338013, 0.8715000152587891, 0.8831400275230408, 0.8888400197029114, 0.8923799991607666, 0.8921599984169006, 0.9009600281715393, 0.8993200063705444, 0.8999800086021423, 0.909280002117157], 52755: [0.833899974822998, 0.8626199960708618, 0.8762000203132629, 0.8879799842834473, 0.8952000141143799, 0.8979799747467041, 0.8968799710273743, 0.8976799845695496, 0.8931599855422974, 0.9022799730300903], 81065: [0.8457199931144714, 0.8731600046157837, 0.8792999982833862, 0.8847799897193909, 0.8920999765396118, 0.8986999988555908, 0.9008600115776062, 0.89274001121521, 0.9033600091934204, 0.8994600176811218]}, 'val_acc_list_dict': {77001: [0.8427, 0.8579, 0.8663, 0.8721, 0.8787, 0.8719, 0.8807, 0.8793, 0.8832, 0.884], 52755: [0.8304, 0.8507, 0.8594, 0.8697, 0.8765, 0.8786, 0.8798, 0.8813, 0.8742, 0.8811], 81065: [0.8416, 0.8597, 0.8665, 0.8681, 0.8747, 0.8763, 0.8782, 0.8756, 0.8786, 0.8779]}, 'train_cce_list_dict': {77001: [0.004317352294921875, 0.0031137924194335935, 0.0027720809936523437, 0.0025820327758789064, 0.0024218844604492187, 0.0023418994140625, 0.0022235017395019533, 0.0021457798767089845, 0.0020938906860351564, 0.002014486389160156], 52755: [0.004350304870605469, 0.0031148712158203125, 0.00279951416015625, 0.0025889529418945314, 0.0024665768432617185, 0.002337578582763672, 0.002248544769287109, 0.0021396717834472655, 0.002056458435058594, 0.0019872297668457033], 81065: [0.004414635620117188, 0.003104010009765625, 0.0028212384033203127, 0.0026139642333984376, 0.0024749552917480468, 0.0023675563049316405, 0.002251977996826172, 0.0021700347900390623, 0.002065221862792969, 0.0019999238586425783]}}, 'postact': {'test_cce_list': [0.0014517539978027344, 0.0015488698959350586, 0.0015667022705078124], 'test_acc_list': [0.8696, 0.8642, 0.8622], 'cputime_list': [113.14244270324707, 112.91542744636536, 113.20689487457275], 'train_acc_list_dict': {77001: [0.8404800295829773, 0.8661199808120728, 0.8763599991798401, 0.8823400139808655, 0.8924800157546997, 0.8938800096511841, 0.8962200284004211, 0.9014400243759155, 0.9012200236320496, 0.90065997838974], 52755: [0.8472399711608887, 0.868120014667511, 0.8826000094413757, 0.8898199796676636, 0.8894199728965759, 0.8966599702835083, 0.8993800282478333, 0.902400016784668, 0.9037799835205078, 0.9015600085258484], 81065: [0.8521000146865845, 0.8740599751472473, 0.8789200186729431, 0.8889399766921997, 0.8895000219345093, 0.8960199952125549, 0.8973399996757507, 0.8958399891853333, 0.9022799730300903, 0.8995199799537659]}, 'val_acc_list_dict': {77001: [0.7907, 0.8252, 0.844, 0.8547, 0.8673, 0.868, 0.8665, 0.867, 0.8743, 0.8765], 52755: [0.8137, 0.841, 0.8547, 0.8622, 0.8578, 0.8675, 0.8676, 0.8741, 0.8767, 0.8736], 81065: [0.7941, 0.8209, 0.8268, 0.8479, 0.8454, 0.8562, 0.8629, 0.8622, 0.8719, 0.8666]}, 'train_cce_list_dict': {77001: [0.00470559814453125, 0.0031426089477539065, 0.0028356143188476564, 0.002629926452636719, 0.002500733184814453, 0.0023918373107910157, 0.002278388366699219, 0.0021790437316894532, 0.0021119049072265625, 0.0020265940856933596], 52755: [0.00434407958984375, 0.00311839111328125, 0.0028050027465820315, 0.002594745178222656, 0.0024711099243164064, 0.0023571014404296874, 0.002234974365234375, 0.002129510955810547, 0.002051008453369141, 0.00197765869140625], 81065: [0.004507053527832031, 0.0031397256469726563, 0.0028085311889648437, 0.0026214443969726563, 0.002468592529296875, 0.002346569061279297, 0.0022278785705566405, 0.0021207963562011717, 0.0020305458068847655, 0.001962476501464844]}}}\n",
            "Inference stat dict for optimizer : SGD: \n",
            "{'preact': {'test_cce_list': [0.0013803061485290528, 0.0014349761009216308, 0.0014285715103149414], 'test_acc_list': [0.8744, 0.8731, 0.8681], 'cputime_list': [113.8013436794281, 113.61838626861572, 113.44157814979553], 'train_acc_list_dict': {77001: [0.8468199968338013, 0.8715000152587891, 0.8831400275230408, 0.8888400197029114, 0.8923799991607666, 0.8921599984169006, 0.9009600281715393, 0.8993200063705444, 0.8999800086021423, 0.909280002117157], 52755: [0.833899974822998, 0.8626199960708618, 0.8762000203132629, 0.8879799842834473, 0.8952000141143799, 0.8979799747467041, 0.8968799710273743, 0.8976799845695496, 0.8931599855422974, 0.9022799730300903], 81065: [0.8457199931144714, 0.8731600046157837, 0.8792999982833862, 0.8847799897193909, 0.8920999765396118, 0.8986999988555908, 0.9008600115776062, 0.89274001121521, 0.9033600091934204, 0.8994600176811218]}, 'val_acc_list_dict': {77001: [0.8427, 0.8579, 0.8663, 0.8721, 0.8787, 0.8719, 0.8807, 0.8793, 0.8832, 0.884], 52755: [0.8304, 0.8507, 0.8594, 0.8697, 0.8765, 0.8786, 0.8798, 0.8813, 0.8742, 0.8811], 81065: [0.8416, 0.8597, 0.8665, 0.8681, 0.8747, 0.8763, 0.8782, 0.8756, 0.8786, 0.8779]}, 'train_cce_list_dict': {77001: [0.004317352294921875, 0.0031137924194335935, 0.0027720809936523437, 0.0025820327758789064, 0.0024218844604492187, 0.0023418994140625, 0.0022235017395019533, 0.0021457798767089845, 0.0020938906860351564, 0.002014486389160156], 52755: [0.004350304870605469, 0.0031148712158203125, 0.00279951416015625, 0.0025889529418945314, 0.0024665768432617185, 0.002337578582763672, 0.002248544769287109, 0.0021396717834472655, 0.002056458435058594, 0.0019872297668457033], 81065: [0.004414635620117188, 0.003104010009765625, 0.0028212384033203127, 0.0026139642333984376, 0.0024749552917480468, 0.0023675563049316405, 0.002251977996826172, 0.0021700347900390623, 0.002065221862792969, 0.0019999238586425783]}}, 'postact': {'test_cce_list': [0.0014517539978027344, 0.0015488698959350586, 0.0015667022705078124], 'test_acc_list': [0.8696, 0.8642, 0.8622], 'cputime_list': [113.14244270324707, 112.91542744636536, 113.20689487457275], 'train_acc_list_dict': {77001: [0.8404800295829773, 0.8661199808120728, 0.8763599991798401, 0.8823400139808655, 0.8924800157546997, 0.8938800096511841, 0.8962200284004211, 0.9014400243759155, 0.9012200236320496, 0.90065997838974], 52755: [0.8472399711608887, 0.868120014667511, 0.8826000094413757, 0.8898199796676636, 0.8894199728965759, 0.8966599702835083, 0.8993800282478333, 0.902400016784668, 0.9037799835205078, 0.9015600085258484], 81065: [0.8521000146865845, 0.8740599751472473, 0.8789200186729431, 0.8889399766921997, 0.8895000219345093, 0.8960199952125549, 0.8973399996757507, 0.8958399891853333, 0.9022799730300903, 0.8995199799537659]}, 'val_acc_list_dict': {77001: [0.7907, 0.8252, 0.844, 0.8547, 0.8673, 0.868, 0.8665, 0.867, 0.8743, 0.8765], 52755: [0.8137, 0.841, 0.8547, 0.8622, 0.8578, 0.8675, 0.8676, 0.8741, 0.8767, 0.8736], 81065: [0.7941, 0.8209, 0.8268, 0.8479, 0.8454, 0.8562, 0.8629, 0.8622, 0.8719, 0.8666]}, 'train_cce_list_dict': {77001: [0.00470559814453125, 0.0031426089477539065, 0.0028356143188476564, 0.002629926452636719, 0.002500733184814453, 0.0023918373107910157, 0.002278388366699219, 0.0021790437316894532, 0.0021119049072265625, 0.0020265940856933596], 52755: [0.00434407958984375, 0.00311839111328125, 0.0028050027465820315, 0.002594745178222656, 0.0024711099243164064, 0.0023571014404296874, 0.002234974365234375, 0.002129510955810547, 0.002051008453369141, 0.00197765869140625], 81065: [0.004507053527832031, 0.0031397256469726563, 0.0028085311889648437, 0.0026214443969726563, 0.002468592529296875, 0.002346569061279297, 0.0022278785705566405, 0.0021207963562011717, 0.0020305458068847655, 0.001962476501464844]}}}\n",
            "Current status of dictionary: {'SGD': {'preact': {'test_cce_list': [0.0013803061485290528, 0.0014349761009216308, 0.0014285715103149414], 'test_acc_list': [0.8744, 0.8731, 0.8681], 'cputime_list': [113.8013436794281, 113.61838626861572, 113.44157814979553], 'train_acc_list_dict': {77001: [0.8468199968338013, 0.8715000152587891, 0.8831400275230408, 0.8888400197029114, 0.8923799991607666, 0.8921599984169006, 0.9009600281715393, 0.8993200063705444, 0.8999800086021423, 0.909280002117157], 52755: [0.833899974822998, 0.8626199960708618, 0.8762000203132629, 0.8879799842834473, 0.8952000141143799, 0.8979799747467041, 0.8968799710273743, 0.8976799845695496, 0.8931599855422974, 0.9022799730300903], 81065: [0.8457199931144714, 0.8731600046157837, 0.8792999982833862, 0.8847799897193909, 0.8920999765396118, 0.8986999988555908, 0.9008600115776062, 0.89274001121521, 0.9033600091934204, 0.8994600176811218]}, 'val_acc_list_dict': {77001: [0.8427, 0.8579, 0.8663, 0.8721, 0.8787, 0.8719, 0.8807, 0.8793, 0.8832, 0.884], 52755: [0.8304, 0.8507, 0.8594, 0.8697, 0.8765, 0.8786, 0.8798, 0.8813, 0.8742, 0.8811], 81065: [0.8416, 0.8597, 0.8665, 0.8681, 0.8747, 0.8763, 0.8782, 0.8756, 0.8786, 0.8779]}, 'train_cce_list_dict': {77001: [0.004317352294921875, 0.0031137924194335935, 0.0027720809936523437, 0.0025820327758789064, 0.0024218844604492187, 0.0023418994140625, 0.0022235017395019533, 0.0021457798767089845, 0.0020938906860351564, 0.002014486389160156], 52755: [0.004350304870605469, 0.0031148712158203125, 0.00279951416015625, 0.0025889529418945314, 0.0024665768432617185, 0.002337578582763672, 0.002248544769287109, 0.0021396717834472655, 0.002056458435058594, 0.0019872297668457033], 81065: [0.004414635620117188, 0.003104010009765625, 0.0028212384033203127, 0.0026139642333984376, 0.0024749552917480468, 0.0023675563049316405, 0.002251977996826172, 0.0021700347900390623, 0.002065221862792969, 0.0019999238586425783]}}, 'postact': {'test_cce_list': [0.0014517539978027344, 0.0015488698959350586, 0.0015667022705078124], 'test_acc_list': [0.8696, 0.8642, 0.8622], 'cputime_list': [113.14244270324707, 112.91542744636536, 113.20689487457275], 'train_acc_list_dict': {77001: [0.8404800295829773, 0.8661199808120728, 0.8763599991798401, 0.8823400139808655, 0.8924800157546997, 0.8938800096511841, 0.8962200284004211, 0.9014400243759155, 0.9012200236320496, 0.90065997838974], 52755: [0.8472399711608887, 0.868120014667511, 0.8826000094413757, 0.8898199796676636, 0.8894199728965759, 0.8966599702835083, 0.8993800282478333, 0.902400016784668, 0.9037799835205078, 0.9015600085258484], 81065: [0.8521000146865845, 0.8740599751472473, 0.8789200186729431, 0.8889399766921997, 0.8895000219345093, 0.8960199952125549, 0.8973399996757507, 0.8958399891853333, 0.9022799730300903, 0.8995199799537659]}, 'val_acc_list_dict': {77001: [0.7907, 0.8252, 0.844, 0.8547, 0.8673, 0.868, 0.8665, 0.867, 0.8743, 0.8765], 52755: [0.8137, 0.841, 0.8547, 0.8622, 0.8578, 0.8675, 0.8676, 0.8741, 0.8767, 0.8736], 81065: [0.7941, 0.8209, 0.8268, 0.8479, 0.8454, 0.8562, 0.8629, 0.8622, 0.8719, 0.8666]}, 'train_cce_list_dict': {77001: [0.00470559814453125, 0.0031426089477539065, 0.0028356143188476564, 0.002629926452636719, 0.002500733184814453, 0.0023918373107910157, 0.002278388366699219, 0.0021790437316894532, 0.0021119049072265625, 0.0020265940856933596], 52755: [0.00434407958984375, 0.00311839111328125, 0.0028050027465820315, 0.002594745178222656, 0.0024711099243164064, 0.0023571014404296874, 0.002234974365234375, 0.002129510955810547, 0.002051008453369141, 0.00197765869140625], 81065: [0.004507053527832031, 0.0031397256469726563, 0.0028085311889648437, 0.0026214443969726563, 0.002468592529296875, 0.002346569061279297, 0.0022278785705566405, 0.0021207963562011717, 0.0020305458068847655, 0.001962476501464844]}}}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Obtaining inference statistics"
      ],
      "metadata": {
        "id": "M2eEDS6NTtV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the generated JSON for further processing\n",
        "# Note that I have renamed and uploaded the file due to colab disconnects\n",
        "import json\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "fname = \"fmnist_stats_bnorm.json\"\n",
        "with open(fname) as f:\n",
        "        data = json.load(f)"
      ],
      "metadata": {
        "id": "oMJjD8ys0TYR"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "zYHwrL381GHj"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Code to fetch CPU times while training and average cpu time across trials\n",
        "df = pd.DataFrame()\n",
        "\n",
        "optimizer_list = [\"SGD\"]\n",
        "reg_list = [\"preact\", \"postact\"]\n",
        "\n",
        "avg_cputime_dict = {}\n",
        "for optim in optimizer_list:\n",
        "  for reg in reg_list:\n",
        "    key  = f\"{optim}_{reg}\"\n",
        "    df[key] = data[optim][reg][\"cputime_list\"]\n",
        "    avg_cputime_dict[key + \"_avg\"] = np.mean(df[key])\n",
        "print (\"##CPU time stats across trials##\")\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "id": "-mA7_W2O0Gl-",
        "outputId": "f072aef0-4737-48cc-a18c-bdae6b2e8d8f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##CPU time stats across trials##\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   SGD_preact  SGD_postact\n",
              "0  113.801344   113.142443\n",
              "1  113.618386   112.915427\n",
              "2  113.441578   113.206895"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5e79c298-6fde-4ac6-9b69-2961dbade9be\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SGD_preact</th>\n",
              "      <th>SGD_postact</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>113.801344</td>\n",
              "      <td>113.142443</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>113.618386</td>\n",
              "      <td>112.915427</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>113.441578</td>\n",
              "      <td>113.206895</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5e79c298-6fde-4ac6-9b69-2961dbade9be')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5e79c298-6fde-4ac6-9b69-2961dbade9be button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5e79c298-6fde-4ac6-9b69-2961dbade9be');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (\"#Average CPU time per model#\")\n",
        "df = pd.DataFrame(avg_cputime_dict,index = [0])\n",
        "print (df.iloc[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVtMxeN40MKT",
        "outputId": "c71cbcfe-5549-4e58-f9e9-6bc6c3f006b9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#Average CPU time per model#\n",
            "SGD_preact_avg     113.620436\n",
            "SGD_postact_avg    113.088255\n",
            "Name: 0, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code to obtain average accuracies in test data across trials"
      ],
      "metadata": {
        "id": "0VR1_B9H2D3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame()\n",
        "\n",
        "optimizer_list = [\"SGD\"]\n",
        "reg_list = [\"preact\", \"postact\"]\n",
        "\n",
        "avg_acc_dict = {}\n",
        "for optim in optimizer_list:\n",
        "        for reg in reg_list:\n",
        "                key  = f\"{optim}_{reg}\"\n",
        "                df[key] = data[optim][reg][\"test_acc_list\"]\n",
        "                avg_acc_dict[key + \"_avg\"] = np.mean(df[key])\n",
        "print (\"##Accuracy stats across trials##\")\n",
        "print (df)\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "print (\"#Average Accuracy per model#\")\n",
        "df = pd.DataFrame(avg_acc_dict,index = [0])\n",
        "print (df.iloc[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNXUY4dp1vw4",
        "outputId": "e6a09e56-8d73-4159-c9a6-02cdcd6a38fd"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##Accuracy stats across trials##\n",
            "   SGD_preact  SGD_postact\n",
            "0      0.8744       0.8696\n",
            "1      0.8731       0.8642\n",
            "2      0.8681       0.8622\n",
            "\n",
            "\n",
            "\n",
            "#Average Accuracy per model#\n",
            "SGD_preact_avg     0.871867\n",
            "SGD_postact_avg    0.865333\n",
            "Name: 0, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# average variance in loss(cce) across epochs across seeds for every model training"
      ],
      "metadata": {
        "id": "NW9Trt0B2cJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame()\n",
        "#seeds =\n",
        "variance_dict = {}\n",
        "for optim in optimizer_list:\n",
        "        for reg in reg_list:\n",
        "                key  = f\"{optim}_{reg}\"\n",
        "                train_cce_list_dict = data[optim][reg][\"train_cce_list_dict\"]\n",
        "                avg_cce_list = []\n",
        "                for seed in train_cce_list_dict.keys():\n",
        "                        avg_cce_list.append(np.mean(train_cce_list_dict[seed]))\n",
        "                variance_dict[key] = np.var(avg_cce_list)\n",
        "\n",
        "print (\"#Average Variance in loss across seeds per model#\")\n",
        "df = pd.DataFrame(variance_dict,index = [0])\n",
        "print (df.iloc[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyA4eTqU2RBn",
        "outputId": "524623d9-7714-49d7-ef9c-99f465c358aa"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#Average Variance in loss across seeds per model#\n",
            "SGD_preact     1.194319e-10\n",
            "SGD_postact    9.581537e-10\n",
            "Name: 0, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JIG2VLOC2kUz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "MLP_Fmnist_BatchNorm_Moving.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}